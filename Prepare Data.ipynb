{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import re\n",
    "import pytz\n",
    "# import deepcut\n",
    "from datetime import datetime, timezone\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.corpus.common import thai_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(tweets_data_path):\n",
    "    \"\"\" Function read data. \"\"\"\n",
    "    tweets_data = []\n",
    "    tweets_file = open(tweets_data_path, \"r\")\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    tweets_file.close()\n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data into a list tweets_data.\n",
    "# tweets_data = read_data('data/28-09-62.txt')\n",
    "# tweets_data = read_data('data/28-09-62-2.txt')\n",
    "tweets_data = read_data('data/28-09-62-3.txt')\n",
    "# tweets_data = read_data('data/29-09-62.txt')\n",
    "# tweets_data = read_data('data/29-09-62-2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tweets_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat time and convert time zones UTC To ICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_tz = pytz.timezone('Asia/Bangkok') # thai time zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find error in data\n",
    "lst_error = []\n",
    "for i in range(len(tweets_data)):\n",
    "    try:\n",
    "        tweets_data[i]['created_at'].split(' ')\n",
    "    except:\n",
    "        lst_error.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst_error)\n",
    "print('----------')\n",
    "print(len(lst_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweets_data)):\n",
    "    if i not in lst_error:\n",
    "        lst = tweets_data[i]['created_at'].split(' ')\n",
    "        date = (lst[5]+'.'+lst[1]+'.'+lst[2]+' '+lst[3])\n",
    "        date_time = datetime.strptime(date, '%Y.%b.%d %H:%M:%S')\n",
    "        tweets_data[i]['created_at'] =  date_time.replace(tzinfo=timezone.utc).astimezone(thai_tz).strptime(date, '%Y.%b.%d %H:%M:%S') # reset info and convert to thai timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [tweets_data[i]['created_at'] for i in range(len(tweets_data)) if i not in lst_error]\n",
    "language = [tweets_data[i]['lang'] for i in range(len(tweets_data)) if i not in lst_error]\n",
    "source = [tweets_data[i]['source'][tweets_data[i]['source'].find('w\">')+3:].replace('</a>', '') for i in range(len(tweets_data)) if i not in lst_error]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'time':time, 'language':language, 'source':source}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add group column into data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(0, 25*3600, 1*3600)) # Divide the time into 24 bin ( 1 hours / bin )\n",
    "labels = ['0 pm','1 am','2 am','3 am','4 am',\n",
    "          '5 am','6 am','7 am','8 am','9 am',\n",
    "          '10 am','11 am','12 am','13 pm','14 pm',\n",
    "         '15 pm','16 pm','17 pm','18 pm','19 pm',\n",
    "          '20 pm','21 pm','22 pm','23 pm'] # set label\n",
    "# print(list(range(0, 25, 2)))\n",
    "# bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sec'] = df.time.dt.hour * 3600 + df.time.dt.minute*60 + df.time.dt.second # convert time unit to second.\n",
    "df['group'] = pd.cut(df['sec'], bins=bins, labels=labels) # set bin label into group column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('sec', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_all = []\n",
    "for j in range(len(tweets_data)):\n",
    "    if j not in lst_error:\n",
    "        lst = []\n",
    "        for i in range(len(tweets_data[j]['entities']['hashtags'])):\n",
    "            lst.append(tweets_data[j]['entities']['hashtags'][i]['text'])\n",
    "        hashtag_all.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtag_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtag'] = hashtag_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alltext = []\n",
    "\n",
    "# try: # Check error in data\n",
    "#     for i in range(len(tweets_data)):\n",
    "#         if i not in lst_error: # ensure that message, not in the list of error.\n",
    "#             if re.search('Microsoft PowerApps and Flow',tweets_data[i]['source']): # if tweet posted by Microsoft PowerApps, the key for access text will have only one then this condition must be checked first.\n",
    "#                 text = tweets_data[i]['text']\n",
    "#                 alltext.append(text)\n",
    "#             elif re.search('^RT' ,tweets_data[i]['text']): # check type of tweet [RT = retweet]\n",
    "#                 if re.search('…$' ,tweets_data[i]['text']): # check the message that full message or summary message.\n",
    "#                     try: # in normal case\n",
    "#                         text = tweets_data[i]['retweeted_status']['extended_tweet']['full_text'] # access text.\n",
    "#                         alltext.append(text) # append to the alltext list.\n",
    "# #                         print(text)\n",
    "# #                         print('--------------------')\n",
    "#                     except: # in case of a full message not in key 'extended_tweet'.\n",
    "#                         text = tweets_data[i]['retweeted_status']['text'] # access text.\n",
    "#                         alltext.append(text) # append to the alltext list.\n",
    "# #                         print(text)\n",
    "# #                         print('--------------------')\n",
    "#                 else:\n",
    "#                     if re.search('… https://' ,tweets_data[i]['text']): # check the text is condensed but the embedded link is unabridged.\n",
    "#                         text = tweets_data[i]['extended_tweet']['full_text'] # access text.\n",
    "#                         alltext.append(text) # append to the alltext list.\n",
    "# #                       print(text)\n",
    "# #                       print('--------------------')\n",
    "#                     else:\n",
    "#                         text = tweets_data[i]['retweeted_status']['text'] # access text.\n",
    "#                         alltext.append(text) # append to the alltext list.\n",
    "# #                     print(text)\n",
    "# #                     print('--------------------')\n",
    "#             else: # in case of normal tweets\n",
    "#                 if re.search('… https//' ,tweets_data[i]['text']): # check the text is condensed but the embedded link is unabridged.\n",
    "#                     text = tweets_data[i]['extended_tweet']['full_text'] # access text.\n",
    "#                     alltext.append(text) # append to the alltext list.\n",
    "# #                     print(text)\n",
    "# #                     print('--------------------')\n",
    "#                 else:\n",
    "#                     text = tweets_data[i]['text'] # access text.\n",
    "#                     alltext.append(text) # append to the alltext list.\n",
    "# #                     print(text)\n",
    "# #                     print('--------------------')\n",
    "# except:\n",
    "#     print(f'ERROR FOUND!! INDEX : {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prepared03.csv', encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('prepared01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
